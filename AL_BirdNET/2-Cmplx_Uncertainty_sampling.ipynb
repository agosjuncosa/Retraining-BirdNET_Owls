{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17874ed6-76a0-4751-a1e6-b00e2d19b30d",
   "metadata": {},
   "source": [
    "### Uncertainty sampling with filtering & class-specific dynamic adjustments "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e60e42-312f-4416-8a20-6866777d31fb",
   "metadata": {},
   "source": [
    "We do not need to concatenate the .txt files as all predictions are now condensed into one single big .txt file. \n",
    "We need to select 32 top scoring clips for each class.\n",
    "On iteration 0 it is easy, there is no need of using a filter because there are no clips from field data already incorporated. There is no need to check class precision on previous evaluation as there are no previous calculations yet. So iteration 0 (base) it is easy nd probably it is better to use the base code. **Just select 32 top scoring clips per class.**\n",
    "\n",
    "For the subsequent iterations 1 onwards there will be a need to include a df to filter out the files already included as part of the training classes. Moreover there will be a need to include a vector with the precisions of each class (computed before). If precision of a certain class is below 0.5 (>= 50%), then select only the 32 top scoring clips (filtering out the already included clips). If precision of a certain class was above 0.5 (<= 50%), then select the 64 top scoring clips for that class. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53e7f78-ad63-4317-b6c1-ecea5b05195b",
   "metadata": {},
   "source": [
    "Goals:\n",
    "Select top-scoring 32 (or 64) clips per class for validation.\n",
    "Adjust the number of clips based on the previous iteration's precision:\n",
    "Classes with precision > 50%: Select 64 clips, split (manually?) into:\n",
    "Top 32 for computing precision.\n",
    "Next 32 for potential inclusion in the training set.\n",
    "Classes with precision ≤ 50%: Select 32 clips.\n",
    "Filter out clips already in the training set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee80890-dcc6-4035-ba17-9de73cbbf266",
   "metadata": {},
   "source": [
    "#### Load Required Data:\n",
    "\n",
    "* Combined predictions dataframe (big_df).\n",
    "* training_set_tracker _to exclude already training set clips_.\n",
    "* Class precision dataframe _from the previous iteration_."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "827a6e4d-dc6e-4972-bd5f-b977c7b79c2b",
   "metadata": {},
   "source": [
    "#### Sampling Logic:\n",
    "\n",
    "* For each class:\n",
    "    - Exclude filenames present in the tracker for that class.\n",
    "    - Sort predictions by score and select:\n",
    "        Top 32 if precision ≤ 50%.\n",
    "        Top 64 if precision > 50% (split into two groups).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4223c5c-c2c7-4df4-9d55-1fb1a4d5ecb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "OpenSoundscape",
   "language": "python",
   "name": "opensoundscape"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
